{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42b7fd4",
   "metadata": {},
   "source": [
    "# CIFAR-10 CNN Training\n",
    "\n",
    "This notebook trains and compares CNN architectures on CIFAR-10.\n",
    "All implementation lives in `src/`; this file is only the experiment driver.\n",
    "\n",
    "| Cell group | Purpose |\n",
    "|---|---|\n",
    "| 0 – Environment setup | Colab/Drive mount, repo clone, pip install |\n",
    "| 1 – Imports & config  | Load modules, set hyper-params |\n",
    "| 2 – Data              | Download / inspect CIFAR-10 |\n",
    "| 3 – Single experiment | Train one chosen architecture |\n",
    "| 4 – Comparison        | Train all architectures & compare |\n",
    "| 5 – Evaluation        | Test-set accuracy + per-class breakdown |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bc7a5",
   "metadata": {},
   "source": [
    "## 0 · Environment Setup (Colab)\n",
    "\n",
    "> **Skip this section if you are running locally.**\n",
    ">\n",
    "> When connecting from VS Code to a Colab runtime:\n",
    "> 1. Open the Command Palette → *\"Jupyter: Specify Jupyter Server for Connections\"*\n",
    "> 2. Paste the Colab runtime URL (copy from *Connect → Copy link*).\n",
    "> 3. Make sure you have the **Jupyter** and **Python** VS Code extensions installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # --- Mount Google Drive (optional: for persisting checkpoints) ---\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # --- Clone your repository ---\n",
    "    REPO_URL = \"https://github.com/YOUR_USERNAME/PartInterviewTasks.git\"  # <-- update this\n",
    "    REPO_DIR = \"/content/PartInterviewTasks\"\n",
    "\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        os.system(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "    else:\n",
    "        os.system(f\"cd {REPO_DIR} && git pull\")\n",
    "\n",
    "    os.chdir(REPO_DIR)\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "    # --- Install dependencies ---\n",
    "    os.system(\"pip install -q torch torchvision matplotlib\")\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"In Colab:\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83252db9",
   "metadata": {},
   "source": [
    "## 1 · Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
    "\n",
    "from src.config  import ExperimentConfig, DataConfig, TrainConfig\n",
    "from src.dataset import get_dataloaders, CLASSES\n",
    "from src.models  import get_model, count_parameters\n",
    "from src.trainer import train, evaluate, load_checkpoint\n",
    "from src.utils   import set_seed, get_device, plot_history, compare_histories\n",
    "\n",
    "# ---------- Experiment configuration ----------\n",
    "cfg = ExperimentConfig(\n",
    "    seed       = 42,\n",
    "    model_name = \"SimpleCNN\",   # change to \"DeepCNN\" or \"ResNetCIFAR\"\n",
    "    data  = DataConfig(\n",
    "        batch_size  = 128,\n",
    "        val_split   = 0.1,\n",
    "        augment     = True,\n",
    "        num_workers = 2,\n",
    "    ),\n",
    "    train = TrainConfig(\n",
    "        epochs        = 50,\n",
    "        learning_rate = 1e-3,\n",
    "        weight_decay  = 5e-4,\n",
    "        use_one_cycle = True,\n",
    "        max_lr        = 1e-2,\n",
    "        use_amp       = True,\n",
    "        checkpoint_dir = \"./checkpoints\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6301e6c",
   "metadata": {},
   "source": [
    "## 2 · Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c469443",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_dir    = cfg.data.data_dir,\n",
    "    batch_size  = cfg.data.batch_size,\n",
    "    val_split   = cfg.data.val_split,\n",
    "    augment     = cfg.data.augment,\n",
    "    num_workers = cfg.data.num_workers,\n",
    ")\n",
    "\n",
    "print(f\"Train batches : {len(train_loader)}  ({len(train_loader.dataset)} samples)\")\n",
    "print(f\"Val   batches : {len(val_loader)}  ({len(val_loader.dataset)} samples)\")\n",
    "print(f\"Test  batches : {len(test_loader)}  ({len(test_loader.dataset)} samples)\")\n",
    "print(f\"Classes : {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity-check: visualise a mini-batch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "CIFAR_MEAN = np.array([0.4914, 0.4822, 0.4465])\n",
    "CIFAR_STD  = np.array([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs_show = imgs[:16].cpu().numpy()              # (16, 3, 32, 32)\n",
    "imgs_show = imgs_show.transpose(0, 2, 3, 1)     # (16, 32, 32, 3)\n",
    "imgs_show = imgs_show * CIFAR_STD + CIFAR_MEAN  # un-normalise\n",
    "imgs_show = imgs_show.clip(0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for ax, img, lbl in zip(axes.flat, imgs_show, labels):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(CLASSES[lbl], fontsize=7)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample training images (after augmentation, un-normalised)\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c586d",
   "metadata": {},
   "source": [
    "## 3 · Train a Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(cfg.model_name).to(device)\n",
    "print(f\"Model         : {cfg.model_name}\")\n",
    "print(f\"Parameters    : {count_parameters(model):,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.train.learning_rate, weight_decay=cfg.train.weight_decay)\n",
    "\n",
    "if cfg.train.use_one_cycle:\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr       = cfg.train.max_lr,\n",
    "        epochs       = cfg.train.epochs,\n",
    "        steps_per_epoch = len(train_loader),\n",
    "    )\n",
    "else:\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.train.epochs)\n",
    "\n",
    "history = train(\n",
    "    model          = model,\n",
    "    train_loader   = train_loader,\n",
    "    val_loader     = val_loader,\n",
    "    criterion      = criterion,\n",
    "    optimizer      = optimizer,\n",
    "    scheduler      = scheduler,\n",
    "    device         = device,\n",
    "    epochs         = cfg.train.epochs,\n",
    "    checkpoint_dir = cfg.train.checkpoint_dir,\n",
    "    model_name     = cfg.model_name,\n",
    "    use_amp        = cfg.train.use_amp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, title=cfg.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a8078",
   "metadata": {},
   "source": [
    "## 4 · Compare All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURES = [\"SimpleCNN\", \"DeepCNN\", \"ResNetCIFAR\"]\n",
    "all_histories: dict = {}\n",
    "\n",
    "for arch in ARCHITECTURES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Training: {arch}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    m = get_model(arch).to(device)\n",
    "    print(f\"  Parameters: {count_parameters(m):,}\")\n",
    "\n",
    "    opt  = AdamW(m.parameters(), lr=cfg.train.learning_rate, weight_decay=cfg.train.weight_decay)\n",
    "    sched = OneCycleLR(\n",
    "        opt,\n",
    "        max_lr          = cfg.train.max_lr,\n",
    "        epochs          = cfg.train.epochs,\n",
    "        steps_per_epoch = len(train_loader),\n",
    "    )\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    hist = train(\n",
    "        model          = m,\n",
    "        train_loader   = train_loader,\n",
    "        val_loader     = val_loader,\n",
    "        criterion      = crit,\n",
    "        optimizer      = opt,\n",
    "        scheduler      = sched,\n",
    "        device         = device,\n",
    "        epochs         = cfg.train.epochs,\n",
    "        checkpoint_dir = cfg.train.checkpoint_dir,\n",
    "        model_name     = arch,\n",
    "        use_amp        = cfg.train.use_amp,\n",
    "    )\n",
    "    all_histories[arch] = hist\n",
    "\n",
    "compare_histories(all_histories, metric=\"val_acc\")\n",
    "compare_histories(all_histories, metric=\"val_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fd2be",
   "metadata": {},
   "source": [
    "## 5 · Test-Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "criterion_eval = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"{'Architecture':<16} {'Test Acc':>10}\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "for arch in ARCHITECTURES:\n",
    "    ckpt_path = os.path.join(cfg.train.checkpoint_dir, f\"{arch}_best.pt\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"{arch:<16}  checkpoint not found, skipping\")\n",
    "        continue\n",
    "\n",
    "    m = get_model(arch).to(device)\n",
    "    load_checkpoint(m, ckpt_path, device)\n",
    "    _, test_acc = evaluate(m, test_loader, criterion_eval, device)\n",
    "    print(f\"{arch:<16} {test_acc:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy for the best checkpoint\n",
    "import torch\n",
    "\n",
    "BEST_ARCH = \"ResNetCIFAR\"  # change to whichever model won\n",
    "\n",
    "m = get_model(BEST_ARCH).to(device)\n",
    "load_checkpoint(m, os.path.join(cfg.train.checkpoint_dir, f\"{BEST_ARCH}_best.pt\"), device)\n",
    "m.eval()\n",
    "\n",
    "class_correct = torch.zeros(10)\n",
    "class_total   = torch.zeros(10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        preds = m(inputs).argmax(1)\n",
    "        for c in range(10):\n",
    "            mask = targets == c\n",
    "            class_correct[c] += (preds[mask] == c).sum().item()\n",
    "            class_total[c]   += mask.sum().item()\n",
    "\n",
    "print(f\"Per-class accuracy — {BEST_ARCH}\")\n",
    "print(\"-\" * 35)\n",
    "for c, name in enumerate(CLASSES):\n",
    "    acc = 100.0 * class_correct[c] / class_total[c]\n",
    "    print(f\"  {name:<12} {acc:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
